---
title: "Detecting Defiers: An Empirical Investigation"
author: "Ed Jee"
date: "22 January 2019"
abstract: "I present some practical tests for detecting failure of the monotonicity assumption essential for causal identification under the LATE theorem. Simulations show mixed results for the Bayesian and Random Forest methods whilst the traditional practice of checking heterogeneous first stage treatment effects performs best. I apply these tests to a sample of papers in labour economics using instrumental variables estimation with an emphasis on visualisation and exploration rather than rigid adherence to Fisher hypothesis testing methods although these are supplied as well."
output:
    html_document:
      code_folding: hide
      highlight: tango
      theme: journal
      toc: yes
      toc_float: yes
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```




&nbsp;
&nbsp;

The LATE theorem is a cornerstone of identification in economics and labour economists in particular have popularised and preached its virtues. Considering the theorem's importance relatively little effort has been expended ensuring the theorem's assumptions are met in the dataset in question. This paper aims to promote a practical methodology that can be incorporated into current instrumental variables workflow in order to clearly aid researchers in verifying that necessary, but not sufficient, conditions are met empirically in the sample.

Checking that the first stage "goes the right way" through the use of interaction terms or subsampling is common knowledge amongst applied practitioners but rarely discussed in depth. I formalise this notion and present some novel contributions of potential interest to social science researchers using saturated first stage regressions, multiple comparison adjustments and negative log-log $p$-value transforms popularised by genome wide association studies (GWAS) to present visually striking evidence for or against monotonicity. Next, I show how advances in causal econometric inference using machine learning techniques, developed by @Wager_and_Athey, can be adapted to estimate the underlying first-stage heterogeneity and provide an alternative to the saturated regression model.    

# The LATE Theorem


The LATE theorem [@Angrist_Imbens] describes four key assumptions under which instrumental variables regression will identify a causal effect for a specific subpopulation, "compliers", in the presence of heterogeneous effects.

Using the random coefficient notation of Mostly Harmless Econometrics [@MHE] we have a  potential outcome $Y_i(d, z)$ corresponding to an individual $i$ with treatment status $D_i = d$ and instrument value $Z_i = z$. Observed treatment status is:

$$
D_i = D_{0i} + (D_{1i} - D_{0i})Z_i = \pi_0 + \pi_{1i}Z_i + \epsilon_i
$$

where $\pi_{1i} = (D_{1i} - D_{0i})$ is the heterogeneous causal effect of $Z_i$ on $D_i$ 
The LATE assumptions are:

1. Independence. $\{Y_i(D_{1i}, 1), Y_i(D_{0i}, 0), D_{1i}, D_{0i}\}\  \amalg  \ Z_i$.
2. Exclusion. $Y_i(d, 0) = Y_i(d, 1) = Y_{di}  \ \text{for} \ d = 0,1$.
3. First-Stage. $E[D_{1i} - D_{0i}] \neq 0$.
4. Monotonicity. $D_{1i} - D_{0i} \geq 0 \ \forall \ i \ \text{or vice versa.}$^[For simplicity the rest of the paper assumes $D_{1i} - D_{0i} \geq 0$ unless explicitly stated otherwise.]


Under assumptions 1-4:

$$
\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i | Z_i = 1] - E[D_i | z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} > D_{01}] = E[\rho_i | \pi_{1i} > 0]
$$

The Wald estimator is the Local Average Treatment Effect (LATE) - that is, the average treatment effect for the compliers. The monotonicity assumption is key because it rules out "defiers"; those who would have taken the treatment if they hadn't been assigned the instrument but upon instrument assignment are induced not to take the treatment. In the presence of defiers the equation above no longer estimates LATE but some weighted average of the effect for compliers and defiers.

The Wald estimator under assumptions 1-4 is often criticised for lack of external validity - it's hard to argue that the effect for compliers can be generalised to the rest of the population unless the treatment effect is the same for everyone (unlikely) or assignment to complier, defier, always-taker etc. is as good as random (i.e. $corr(\rho_i, \pi_{1i}) = 0$). However, an advantage of LATE is that the estimate is often exactly what we want to know from a policy perspective - a programme's effect on those likely to respond to the programme is more interesting to us than the effect on the entire population if univeral compliance is unlikely. 

On the other hand the Wald estimator in the presence of defiers is near useless to us. Not only does it lack external validity but it fails to uncover a meaningful internally valid estimate. This, in part, motivates the importance of checking monotonicity.


## Literature

There are two papers of direct relevance to monotonicity tests. @kitagawa's test for instrument validity and @RESTAT's.   





# The Tests

## First Stage Interactions

Common practice in applied work involves checking that the regression first stage "goes the same way" for all subgroups present in a dataset. In the simple binary covariate case this involves checking that $\hat{\pi}_{1im} > 0$ for $m = 0,1$ where $m$ is a dummy variable indicating which sub-population the sample is drawn from, male or female say. This is equivalent to running the saturated regression model:
$$
D_i = \pi_0 + \pi_{1i}Z_i + \pi_{2i}m_i + \pi_{3i}(Z_i \times m_i) + \epsilon_i
$$

and testing $\hat{\pi}_{1i} > 0$ and $\hat{\pi}_{1i} + \hat{\pi}_{3i} > 0$. Extending this framework to multiple, continous and binary-valued covariates is relatively straightforward:

$$
D_i = \pi_0 + \pi_{1i}Z_i + \sum_{k=1}^K (\gamma_{ki}x_{ki} +  \delta_{ki}(Z_i \times x_{ki})) + \epsilon_i
$$

where $x_{ki}$ is a vector of $K$ observed covariates. Differentiation with respect to the instrument $Z_i$ gives:

$$
\frac{\partial D_i}{\partial Z_i} = \pi_{1i} + \sum_{k=1}^K \delta_{ki}x_{ki}
$$

and therefore we wish to test hypotheses of the form:

$$
\begin{aligned}
  H_{0i}: \frac{\partial D_i}{\partial Z_i} = \pi_{1i} + \sum_{k=1}^K \delta_{ki}x_{ki} &= 0 \\
  H_{1i}: \frac{\partial D_i}{\partial Z_i} = \pi_{1i} + \sum_{k=1}^K \delta_{ki}x_{ki} &< 0
\end{aligned}
$$
In the binary covariate case we could simply estimate the saturated model and test each partial derivative separately - one for $m = 1$ and another for $m = 0$. Moving to the continuous case is essentially the same however now we have as many partial derivatives we wish to test as datapoints - our hypotheses are indexed by $i$ because LATE theorem assumption 4 require monotonicity for all $i$ - the partial derivative, $\frac{\partial D_i}{\partial Z_i}$ must be positive for all $i$.

Therefore, our monotonicity test neatly corresponds with the set, or complete, null [@cite_Shaffer_or_Westfall-Young_or_someone_here]:

$$
\begin{aligned}
  H_0^C: \cap_{k \in K} \cap_{i \in N} \pi_{1i} + \delta_{ki}x_{ki} &= 0 \\
  H_1^C: \cap_{k \in K} \cap_{i \in N} \pi_{1i} + \delta_{ki}x_{ki} &< 0
\end{aligned}
$$


Any test of this nature immediately faces two problems. First, there must be control of the family wise error rate in the face of $N$ simultaneous statistical inferences and second we must be confident of test power when performing $N$ inferences based off $N$ datapoints.   


## Random Forest
## Simulations


# Data and Replication


## Angrist and Evans 1998

Describe Angrist and Evans paper and what a defier means in this context.

#### 1990 Census Data
```{r data_import}
library(haven)
library(dplyr)
library(broom)
df_90 <- read_sas("Angrist and Evans 90.sas7bdat")
```


Cleaning data according to Angrist and Evans SAS replication codes:
```{r data_cleaning}

clean_angrist_evans_data <- function(dataset){
  df_clean <- dataset %>% 
    filter((AGEM <= 35) & (AGEM >= 21)) %>% 
    filter(KIDCOUNT >= 2) %>% 
    filter(AGE2NDK >= 1) %>% 
    filter(AAGE == "0" & AAGE2ND == "0" & ASEX == "0" & ASEX2ND == "0") %>% 
    mutate(SEXK = as.numeric(SEXK),
           SEX2NDK = as.numeric(SEX2NDK),
           WEEK89D = as.numeric(WEEK89D),
           WEEK89M = as.numeric(WEEK89M),
           FERTIL = as.numeric(FERTIL)) %>% 
    mutate(fertdif = as.numeric(KIDCOUNT) - (FERTIL - 1),
           agefstm = as.numeric(AGEM) - as.numeric(AGEK)) %>% 
    filter(agefstm >= 15) %>% 
    filter(PWGTM1 > 0) %>% 
    mutate(samesex = (SEXK == SEX2NDK),
           morekids = (KIDCOUNT > 2)) %>% 
    rename_all(tolower)
  return(df_clean)
}
df_90_clean <- df_90 %>%
  clean_angrist_evans_data()
rm(df_90)
```

#### 1980 Census Data


```{r cleaning_1980}

df_80 <- read_sas("Angrist and Evans 80.sas7bdat")

df_80_clean <- df_80 %>%
    mutate(SEXK = as.numeric(SEXK),
         SEX2ND = as.numeric(SEX2ND),
         WEEKSD = as.numeric(WEEKSD),
         WEEKSM = as.numeric(WEEKSM),
         YOBM = as.numeric(YOBM),
         AGEQK = as.numeric(AGEQK),
         YOBD = 80 - as.numeric(AGED),
         YOBD = ifelse(QTRBTHD == 0, YOBD, YOBD - 1)) %>% 
  mutate(samesex = (SEXK == SEX2ND),
         morekids = (KIDCOUNT > 2),
         ageqm = 4*(80-YOBM) - as.numeric(QTRBTHM) - 1,
         ageqd = 4*(80 - YOBD) - as.numeric(QTRBKID),
         agefstm = round((ageqm-AGEQK)/4),
         agefstd = round((ageqd - AGEQK)/4),
         QTRMAR = as.numeric(QTRMAR),
         QTRBTHM = as.numeric(QTRBTHM),
         AGEMAR = as.numeric(AGEMAR),
         QTRBKID = as.numeric(QTRBKID),
         FERTIL = as.numeric(FERT)
         ) %>% 
  rename_all(tolower)




df_80_filtered <- df_80_clean %>% 
  filter((agem <= 35) & (agem >= 21)) %>% 
  filter(kidcount >= 2) %>%
  filter(ageq2nd > 4) %>% 
  filter(agefstm >= 15) %>%
  filter(agefstd >= 15 | is.na(agefstd)) %>%
  filter(asex == 0 &
         aage == 0 & 
         aqtrbrth == 0 &
         asex2nd == 0 &
         aage2nd == 0 &
         aqtrbrth == 0)



df_80_filtered_two <- df_80_filtered %>% 
  mutate(
         qtrmar = ifelse(qtrmar > 0, qtrmar - 1, qtrmar),
         yom = ifelse(qtrbthm <= qtrmar, yobm + agemar, yobm+agemar+1),
         dom_q = yom + (qtrmar/4),
         do1b_q = yobk + qtrbkid/4,
         illegit = ifelse(dom_q - do1b_q > 0, 1, 0)
         ) %>%  
  mutate(
         msample = ifelse(
           !is.na(aged) &
           timesmar == 1 &
           marital == 0 &
           illegit == 0 &
           agefstd >= 15 &
           agefstm >= 15,
           1, 0)
         ) %>% 
  filter(msample == 1)

######################

df_80_subset <- df_80_filtered_two %>% 
  rename(kid_age = agek,
         m_age = agem,
         d_age = aged) %>% 
  select(-starts_with("a")) %>% 
  mutate_at(c("racek",
              "birthplk",
              "schoolk",
              "state",
              "spanishm",
              "spanishd",
              "poverty"), factor) %>% 
  mutate_if(is.character, as.numeric)



df_80_final <- df_80_subset %>% 
  mutate(boy1st = (sexk == 0),
         boy2nd = (sex2nd == 0),
         boys2 = (sexk == 0) & (sex2nd == 0),
         girls2 = (sexk == 1) & (sex2nd == 1),
         samesex = boys2 | girls2,
         morekids = kidcount > 2,
         black_m = (racem == 2),
         hisp_m = (racem == 12),
         white_m = (racem == 01),
         other_race_m = 1 - black_m - hisp_m - white_m,
         black_d = (raced == 2),
         hisp_d = (raced == 12),
         white_d = (raced == 1),
         other_race_d = 1 - black_d - hisp_d - white_d,
         worked_m = (weeksm > 0),
         worked_d = (weeksd > 0),
         income_m = 2.099173554*(income1m + pmax(0, income2m)),
         income_d = (income1d + pmax(0, income2d)),
         fam_inc = pmax(faminc*2.099173554, 1),
         nonmoi = fam_inc - income1m*2.099173554,
         nonmomil = log(pmax(1, nonmoi)))
rm(df_80,
   df_80_clean,
   df_80_filtered,
   df_80_filtered_two,
   df_80_subset)

```



## Replicating Results

```{r data_transforming}

transform_clean_angrist_evans <- function(dataset) {
  transformed_df <- dataset %>%
    rename(
      kid_age = agek,
      kid2_age = age2ndk,
      m_age = agem,
      d_age = aged
    ) %>%
    select(-starts_with("a")) %>%
    mutate_at(c(
      "racek",
      "birthplk",
      "schoolk",
      "state",
      "hispm",
      "hispd",
      "poverty",
      "pobm",
      "hispd",
      "pobd",
      "hispk"
    ), factor) %>%
    mutate_if(is.character, as.numeric) %>%
    mutate(
      boy1st = (sexk == 0),
      boy2nd = (sex2ndk == 0),
      boys2 = (sexk == 0) & (sex2ndk == 0),
      girls2 = (sexk == 1) & (sex2ndk == 1),
      samesex = boys2 | girls2,
      morekids = kidcount > 2,
      black_m = (racem == 2),
      hisp_m = (racem == 12),
      white_m = (racem == 01),
      other_race_m = 1 - black_m - hisp_m - white_m,
      black_d = (raced == 2),
      hisp_d = (raced == 12),
      white_d = (raced == 1),
      other_race_d = 1 - black_d - hisp_d - white_d,
      worked_m = (week89m > 0),
      worked_d = (week89d > 0)
    )

  return(transformed_df)
}

df_90_subset <- df_90_clean %>%
  transform_clean_angrist_evans()

  
  
  
df_90_final <- df_90_subset %>% 
  mutate(
         income_m = 1.2883*(incomem1 + pmax(0, incomem2)),
         income_d = (incomed1 + pmax(0, incomed2)),
         fam_inc = pmax(faminc*1.2883, 1),
         nonmoi = fam_inc - incomem1*1.2883,
         nonmomil = log(pmax(1, nonmoi))
  )

rm(df_90_clean, df_90_subset)
```


#### Table Two Summary Statistics

Trying to recreate table 2
```{r table_2}
library(tidyr)
AE_90_summary <- df_90_final %>% 
  summarise(children_ever_born = mean(fertil - 1),
            more_than_2 = mean(morekids),
            mean_boy_first = mean(boy1st),
            boy_2nd = mean(boy2nd),
            two_boys = mean(boys2),
            two_girls = mean(girls2),
            samesex = mean(samesex),
            age = mean(m_age),
            worked = mean(worked_m),
            weeks = mean(week89m),
            hrs_wk = mean(hour89m),
            labour_income_mum = mean(income_m),
            labour_income_dad = mean(income_d, na.rm = TRUE),
            fam_income = mean(fam_inc),
            mean_non_wife = mean(nonmoi)) %>% 
  gather(term, mean_90) 

AE_80_summary <- df_80_final %>% 
    summarise(children_ever_born = mean(fertil - 1),
            more_than_2 = mean(morekids),
            mean_boy_first = mean(boy1st),
            boy_2nd = mean(boy2nd),
            two_boys = mean(boys2),
            two_girls = mean(girls2),
            samesex = mean(samesex),
            age = mean(m_age),
            worked = mean(worked_m),
            weeks = mean(weeksm),
            hrs_wk = mean(hoursm),
            labour_income_mum = mean(income_m),
            labour_income_dad = mean(income_d, na.rm = TRUE),
            fam_income = mean(fam_inc),
            mean_non_wife = mean(nonmoi)) %>% 
  gather(term, mean_80) 

AE_summary <- inner_join(AE_80_summary, AE_90_summary, by = "term") %>% 
  knitr::kable(digits = 3)
AE_summary

```

#### Regression Results

Recreating Table 5
```{r table_5, results = "asis"}
library(AER)
library(stargazer)
library(purrr)
table_5_models_90 <- c("worked_m",
  "week89m",
  "hour89m",
  "income_m",
  "log(fam_inc)") %>%
  map(~(
  paste0(., " ~ morekids | samesex") %>% 
  as.formula() %>% 
  ivreg(., data = df_90_final)))

table_5_models_80 <- c("worked_m",
  "weeksm",
  "hoursm",
  "income_m",
  "log(fam_inc)") %>% 
  map(~(
  paste0(., " ~ morekids | samesex") %>% 
  as.formula() %>% 
  ivreg(., data = df_80_final)))

stargazer(table_5_models_80,
          omit = "Constant",
          title = "WALD ESTIMATES OF LABOR-SUPPLY MODELS 1980 DATA - REPLICATED",header = FALSE,type = "html",
          dep.var.labels = c("Worked",
                            "Weeks",
                            "Hours",
                            "Income",
                            "Family Income"))

stargazer(table_5_models_90,
          omit = "Constant",
          title = "WALD ESTIMATES OF LABOR-SUPPLY MODELS 1990 DATA - REPLICATED",header = FALSE,type = "html",
          dep.var.labels = c("Worked",
                            "Weeks",
                            "Hours",
                            "Income",
                            "Family Income"))





rm(table_5_models_80,
   table_5_models_90)
``` 



Now table 6
```{r table_6, results = "asis"}
create_model_formula <- function(dependent_var){
  model <- as.formula(paste0(dependent_var, "~ morekids + m_age + boy1st + boy2nd + black_m + hisp_m + other_race_m | samesex +  m_age + boy1st + boy2nd + black_m + hisp_m + other_race_m"))
  return(model)
}

table_6_models_80 <- c("worked_m",
  "weeksm",
  "hoursm",
  "income_m",
  "log(fam_inc)") %>% 
  map(~(create_model_formula(.) %>% 
          ivreg(data = df_80_final)))
  

table_6_models_90 <- c("worked_m",
                    "week89m",
                    "hour89m",
                    "income_m",
                    "log(fam_inc)") %>% 
  map(~(create_model_formula(.) %>% 
          ivreg(data = df_90_final)))

stargazer(table_6_models_80,
          keep = "morekidsTRUE",
          title = "2SLS ESTIMATES OF LABOR-SUPPLY MODELS USING 1980 CENSUS DATA - REPLICATION",
          notes = "Age at first birth omitted since I can't seem to find it.",
          dep.var.labels = c("Worked",
                            "Weeks",
                            "Hours",
                            "Income",
                            "Family Income"),
          type = "html")

stargazer(table_6_models_90, keep = "morekidsTRUE",
          title = "2SLS ESTIMATES OF LABOR-SUPPLY MODELS USING 1990 CENSUS DATA - REPLICATION",
          notes = "Age at first birth omitted since I can't seem to find it.",
          dep.var.labels = c("Worked",
                            "Weeks",
                            "Hours",
                            "Income",
                            "Family Income"),
          type = "html")
rm(table_6_models_80, table_6_models_90)
```



## Maimonides Rule


Mention somewhere HAC instead of moulton factor adjustment - only makes a large difference in the discontinuity sample.

#### Cleaning Grade 5
```{r maimonides_grade_5}
clean_maimonides_data <- function(dataset){
  clean_df <- dataset %>%
    mutate(
      avgverb = ifelse(avgverb > 100, avgverb - 100, avgverb),
      avgmath = ifelse(avgmath > 100, avgmath - 100, avgmath),
      func1 = c_size/(as.integer((c_size-1)/40)+1),
      func2 = cohsize/(as.integer(cohsize/40)+1),
      avgverb = ifelse(verbsize == 0, NA, avgverb),
      passverb = ifelse(verbsize == 0, NA, passverb),
      avgmath = ifelse(mathsize == 0, NA, avgmath),
      passmath = ifelse(mathsize == 0, NA, avgmath),
      disc = (c_size >= 36 & c_size <= 45) | 
             (c_size >= 76 & c_size <= 85) |
             (c_size >= 116 & c_size <= 125),
      all = 1,
      c_size_squared = (c_size^2)/100,
      trend = ifelse(c_size >= 0 & c_size <= 40, c_size, NA),
      trend = ifelse(c_size >= 41 & c_size <= 80, 20 + (c_size/2), trend),
      trend = ifelse(c_size >= 81 & c_size <= 120, (100/3) + (c_size/3), trend),
      trend = ifelse(c_size >= 121 & c_size <= 160, (130/3) + (c_size/4), trend)
    ) %>% 
    filter(
      classize > 1 & classize < 45 & c_size > 5
    ) %>% 
    filter(
      c_leom == 1 & c_pik < 3
    )
  return(clean_df)
  
}
df_final5_cleaned <- read_dta("Angrist and Lavy Grade 5.dta") %>% 
  clean_maimonides_data()

##do stuff


```


#### Cleaning Grade 4
```{r maimonides_grade_4}
df_final4_cleaned <- read_dta("Angrist and Lavy Grade 4.dta") %>% 
  clean_maimonides_data()

```


## Replicating OLS 
```{r replicating_maimonides, results = "asis"}
## Grade 5
col_1 <- lm(avgverb ~ classize, data = df_final5_cleaned)
col_2 <- lm(avgverb ~ classize + tipuach, data = df_final5_cleaned)
col_3 <- lm(avgverb ~ classize + tipuach + c_size, data = df_final5_cleaned)
col_4 <- lm(avgmath ~ classize, data = df_final5_cleaned)
col_5 <- lm(avgmath ~ classize + tipuach, data = df_final5_cleaned)
col_6 <- lm(avgmath ~ classize + tipuach + c_size, data = df_final5_cleaned)
## Grade 4
col_7 <- lm(avgverb ~ classize, data = df_final4_cleaned)
col_8 <- lm(avgverb ~ classize + tipuach, data = df_final4_cleaned)
col_9 <- lm(avgverb ~ classize + tipuach + c_size, data = df_final4_cleaned)
col_10 <- lm(avgmath ~ classize, data = df_final4_cleaned)
col_11 <- lm(avgmath ~ classize + tipuach, data = df_final4_cleaned)
col_12 <- lm(avgmath ~ classize + tipuach + c_size, data = df_final4_cleaned)

SEs <- list(col_1,
            col_2,
            col_3,
            col_4,
            col_5,
            col_6,
            col_7,
            col_8,
            col_9,
            col_10,
            col_11,
            col_12) %>% 
  map(~(vcovHAC(.) %>% 
          diag() %>% 
          sqrt))

stargazer(col_1,
          col_2,
          col_3,
          col_4,
          col_5,
          col_6,
          col_7,
          col_8,
          col_9,
          col_10,
          col_11,
          col_12,
          se = SEs,
          omit.table.layout = "sn",
          type = "html",
          omit = "Constant",
          dep.var.labels = c("English Comprehension",
                             "Maths",
                             "English Comprehension",
                             "Maths"),
          column.labels = c("Grade 5",
                            "Grade 4"),
          column.separate = c(6, 6),
          initial.zero = FALSE,
          notes = "HAC standard errors used instead of Moulton factor adjustment.")
rm(col_1,
   col_2,
   col_3,
   col_4,
   col_5,
   col_6,
   col_7,
   col_8,
   col_9,
   col_10,
   col_11,
   col_12
   )
```


## Replicating IV
```{r maimonides_IV, results = "asis"}

## Grade 5


iv_maim_verb_full_5 <- ivreg(avgverb ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final5_cleaned)
iv_maim_math_full_5 <- ivreg(avgmath ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final5_cleaned)

iv_maim_verb_disc_5 <- ivreg(avgverb ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final5_cleaned %>% filter(disc == 1))
iv_maim_math_disc_5 <- ivreg(avgmath ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final5_cleaned %>% filter(disc == 1))


## Grade 4

iv_maim_verb_full_4 <- ivreg(avgverb ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final4_cleaned)
iv_maim_math_full_4 <- ivreg(avgmath ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final4_cleaned)

iv_maim_verb_disc_4 <- ivreg(avgverb ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final4_cleaned %>% filter(disc == 1))
iv_maim_math_disc_4 <- ivreg(avgmath ~ classize + tipuach + c_size + c_size_squared | func1 + tipuach + c_size + c_size_squared, data = df_final4_cleaned %>% filter(disc == 1))




SEs_IV_5 <- list(
  iv_maim_verb_full_5,
  iv_maim_math_full_5,
  iv_maim_verb_disc_5,
  iv_maim_math_disc_5
) %>%
  map(~(vcovHAC(.) %>% 
          diag() %>% 
          sqrt))
stargazer(iv_maim_verb_full_5,
          iv_maim_math_full_5,
          iv_maim_verb_disc_5,
          iv_maim_math_disc_5,
          se = SEs_IV_5,
          column.labels = c("Full Sample", "Discontinuity Sample"),
          column.separate = c(2, 2),
          type = "html",
          title = "IV Maimonides Rule Fifth Grade",
          notes = "HAC standard errors used instead of Moulton factor adjustment.")



SEs_IV_4 <- list(
  iv_maim_verb_full_4,
  iv_maim_math_full_4,
  iv_maim_verb_disc_4,
  iv_maim_math_disc_4
) %>%
  map(~(vcovHAC(.) %>% 
          diag() %>% 
          sqrt))
stargazer(iv_maim_verb_full_4,
          iv_maim_math_full_4,
          iv_maim_verb_disc_4,
          iv_maim_math_disc_4,
          se = SEs_IV_4,
          column.labels = c("Full Sample", "Discontinuity Sample"),
          column.separate = c(2, 2),
          type = "html",
          title = "IV Maimonides Rule Fourth Grade",
          notes = "HAC standard errors used instead of Moulton factor adjustment.")

rm(SEs_IV_4,
   SEs_IV_5,
   SEs,
   iv_maim_math_disc_4,
   iv_maim_math_full_4,
   iv_maim_verb_disc_4,
   iv_maim_verb_full_4,
   iv_maim_math_disc_5,
   iv_maim_math_full_5,
   iv_maim_verb_disc_5,
   iv_maim_verb_full_5)

```


# Checking First Stage



## Simulations

```{r first_stage_sims}
library(margins)
x <- rnorm(100) + 3
D <- rbinom(100, size = 1, 0.5)
y <- D*5*x + (1-D)*-5*x + 10 + rnorm(100)
df_sim <- tibble(y, x, D)
lm(y ~ x, data = df_sim) %>% summary()
D_yes <- lm(y ~ x, data = df_sim %>% filter(D == 1)) 
D_no <- lm(y ~ x, data = df_sim %>% filter(D ==0)) 
D_int <- lm(y ~ x  +D+ x*D, data = df_sim) 

head(dydx(data = df_sim, D_int, "x"))


df_sim_margins <- margins(data = df_sim, D_int, "x", unit_ses = TRUE) %>% 
  as_tibble() %>% 
  mutate(dydx_x_low = dydx_x  - 1.96*SE_dydx_x,
         dydx_high = dydx_x + 1.96*SE_dydx_x)
  
df_sim_margins %>% head()
rm(x,
   D,
   y,
   df_sim,
   D_yes,
   D_no,
   D_int,
   df_sim_margins)

```




```{r first_stage_functions}

##TODO: Propagate dydx to entire dataset?
library(Rfast)

find_SEs <- function(model_data_no_y, model_vcov, instrument){
  model_data_no_y <- model_data_no_y %>% 
    rename("instrument" = instrument)
  model_formula <- as.formula(paste0("~", "instrument", "*."))
  
  model_matrix <- model_data_no_y %>% 
    mutate(instrument = 1) %>% 
    model.matrix(model_formula, data = .)
  
  gradient_matrix_kinda <- model_matrix %>%
    as_tibble() %>% 
  mutate_at(vars(-contains(":")), ~(. = 0)) %>% 
    mutate(instrument = 1) %>% 
  as.matrix()
  
  split_indices <- seq(from = 1, to = nrow(gradient_matrix_kinda), by = 10000) %>% 
    c(nrow(gradient_matrix_kinda))
  
  if (length(split_indices) < 3){
    vcov_dydx_intermediate <- Rfast::mat.mult(gradient_matrix_kinda, model_vcov)
    vcov_dydx <- Rfast::mat.mult(vcov_dydx_intermediate,
                                 Rfast::transpose(gradient_matrix_kinda))
    SE_dydx <- sqrt(diag(vcov_dydx))
    
  } else {
    baby_SE <- matrix(nrow = nrow(gradient_matrix_kinda), ncol = 1)
    for (i in 1:(length(split_indices)-1)){
      baby_matrix <- gradient_matrix_kinda[split_indices[[i]]:split_indices[[i+1]], ]
      print(paste0(split_indices[i],"to", split_indices[i+1]))
      
      baby_vcov <- Rfast::mat.mult(baby_matrix, model_vcov)
      baby_vcov <- Rfast::mat.mult(baby_vcov, Rfast::transpose(baby_matrix))
      baby_SE[split_indices[[i]]:split_indices[[i+1]], ] <- sqrt(diag(baby_vcov))
      i <- i + 1
    }
    SE_dydx <- baby_SE[, 1]
  }

  return(SE_dydx)
    
} 
run_first_stage_interactions_fast <- function(dataset, dependent_variable, instrument, weights = NULL){
  dataset <- dataset %>% 
    rename("instrument" = instrument)
  model_formula <- as.formula(paste0(dependent_variable,  "~ ", "instrument", "*."))
  first_stage_fit <- lm(data = dataset, formula = model_formula, weights = weights)
  degrees_freedom <- first_stage_fit$df.residual
  instrument_dummy_val <- dataset$instrument[1]
  
  dataset_unique <- dataset %>% 
    select(-dependent_variable, -instrument) %>% 
    distinct() %>% 
    mutate(instrument = instrument_dummy_val)
  first_stage_margins <- dydx(model = first_stage_fit, data = dataset_unique, variable = "instrument") 
  df_margins <- bind_cols(first_stage_margins %>% 
                            select(contains("dydx")),
                          dataset_unique) %>% 
    as_tibble()
  df_margins$SE_dydx_instrument <- find_SEs(dataset_unique, vcov(first_stage_fit), instrument)
  
  df_margins <- df_margins %>% 
    mutate(dydx_lo = dydx_instrument - qt(0.975, degrees_freedom) * SE_dydx_instrument,
           dydx_hi = dydx_instrument + qt(0.975, degrees_freedom) * SE_dydx_instrument,
           t_stat = dydx_instrument/SE_dydx_instrument,
           pval_one_neg = pt(t_stat, degrees_freedom),
           pval_holm = p.adjust(pval_one_neg, method = "holm"))
  return(df_margins)
}


```


## Angrist and Evans
```{r run_first_stage_AE}

AE_data_80 <- df_80_final %>% 
  select(
    samesex,
    morekids,
    black_m,
    hisp_m,
    other_race_m,
    black_d,
    hisp_d,
    other_race_d,
    gradem,
    graded,
    m_age,
    d_age
  ) %>% 
  na.omit()

AE_data_90 <- df_90_final %>% 
  select(samesex,
         morekids,
         black_m,
         hisp_m,
         other_race_m,
         black_d,
         hisp_d,
         other_race_d,
         yearschm,
         yearschd, 
         m_age,
         d_age,
         pwgtm1,
         pwgtd1) %>% 
  na.omit()





first_stage_interactions_80 <- AE_data_80 %>% 
  run_first_stage_interactions_fast(dataset = .,
                                    dependent_variable = "morekids",
                                    instrument = "samesex") %>% 
  mutate(dataset = "1980") %>% 
  select(dydx_instrument, SE_dydx_instrument, pval_one_neg, pval_holm,  dataset, dydx_lo, dydx_hi)
first_stage_interactions_90 <- AE_data_90 %>% 
  run_first_stage_interactions_fast(dataset = .,
                                   dependent_variable = "morekids",
                                   instrument = "samesex") %>% 
  mutate(dataset = "1990") %>% 
  select(dydx_instrument, SE_dydx_instrument, pval_one_neg, pval_holm, dataset, dydx_lo, dydx_hi)

first_stage_interactions_AE <- bind_rows(first_stage_interactions_90, first_stage_interactions_80) 



```


testing old stuff
```{r old_stuff, eval = FALSE, echo = FALSE}

run_first_stage_interactions_fast_slow_SE <- function(dataset, dependent_variable, instrument){
  dataset <- dataset %>% 
    rename("instrument" = instrument)
  model_formula <- as.formula(paste0(dependent_variable,  "~ ", "instrument", "*."))
  first_stage_fit <- lm(data = dataset, formula = model_formula)
  degrees_freedom <- first_stage_fit$df.residual
  dydx_margins <- dydx(data = dataset,
                       model = first_stage_fit,
                       variable = "instrument") %>% pull()
  dataset_unique <- dataset %>% 
    select(-dependent_variable, -instrument) %>% 
    distinct() %>% 
    mutate(instrument = TRUE)
  first_stage_margins <- dydx(model = first_stage_fit, data = dataset_unique, variable = "instrument") 
  df_margins <- bind_cols(first_stage_margins %>% 
                            select(contains("dydx")),
                          dataset_unique) %>% 
    as_tibble()
  df_margins$SE_dydx_instrument <- find_SEs_slow(dataset_unique, vcov(first_stage_fit), instrument)
  
  df_margins <- df_margins %>% 
    mutate(dydx_lo = dydx_instrument - qt(0.975, degrees_freedom) * SE_dydx_instrument,
           dydx_hi = dydx_instrument + qt(0.975, degrees_freedom) * SE_dydx_instrument,
           t_stat = dydx_instrument/SE_dydx_instrument,
           pval_one_neg = pt(t_stat, degrees_freedom))
  return(df_margins)
}

find_SEs_slow <- function(model_data_no_y, model_vcov, instrument){
  model_data_no_y <- model_data_no_y %>% 
    rename("instrument" = instrument)
  model_formula <- as.formula(paste0("~", "instrument", "*."))
  
  model_matrix <- model_data_no_y %>% 
    mutate(instrument = 1) %>% 
    model.matrix(model_formula, data = .)
  
  gradient_matrix_kinda <- model_matrix %>%
    as_tibble() %>% 
    distinct() %>% 
  mutate_at(vars(-contains(":")), ~(. = 0)) %>% 
    mutate(instrument = 1) %>% 
  as.matrix()
  
  vcov_dydx_intermediate <- Rfast::mat.mult(gradient_matrix_kinda, model_vcov)
  vcov_dydx <- Rfast::mat.mult(vcov_dydx_intermediate, Rfast::transpose(gradient_matrix_kinda))
  SE_dydx <- sqrt(diag(vcov_dydx))
  return(SE_dydx)
    
} 


run_first_stage_interactions_slow <- function(dataset, dependent_variable, instrument){
  dataset <- dataset %>% 
    rename("instrument" = instrument)
  model_formula <- as.formula(paste0(dependent_variable,  "~ ", "instrument", "*."))
  first_stage_fit <- lm(data = dataset, formula = model_formula)
  degrees_freedom <- first_stage_fit$df.residual
  dydx_margins <- dydx(data = dataset,
                       model = first_stage_fit,
                       variable = "instrument") %>% pull()
  dataset_unique <- dataset %>% 
    select(-dependent_variable, -instrument) %>% 
    distinct() %>% 
    mutate(instrument = TRUE)
  first_stage_margins <- margins(first_stage_fit, data = dataset_unique, variables = "instrument", unit_ses = TRUE) 
  df_margins <- bind_cols(first_stage_margins %>% 
                            select(contains("dydx")),
                          dataset_unique) %>% 
    as_tibble()
  df_margins <- df_margins %>% 
    mutate(dydx_lo = dydx_instrument - qt(0.975, degrees_freedom) * SE_dydx_instrument,
           dydx_hi = dydx_instrument + qt(0.975, degrees_freedom) * SE_dydx_instrument,
           t_stat = dydx_instrument/SE_dydx_instrument,
           pval_one_neg = pt(t_stat, degrees_freedom))
  return(df_margins)
}



test <- test_data %>% 
  run_first_stage_interactions_fast_slow_SE(dataset = ., 
                                    dependent_variable = "morekids",
                                    instrument = "samesex")

all_equal(first_stage_interactions, test)
```



```{r plotting_first_stage_interactions}
library(ggplot2)
library(scales)
first_stage_interactions_AE %>% 
  filter(pval_one_neg < 0.1 | pval_holm < 0.1)

first_stage_interactions_AE %>% 
  ggplot(aes(x = dydx_instrument, fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")

first_stage_interactions_AE %>% 
  ggplot(aes(x = pval_one_neg, fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")

first_stage_interactions_AE %>% 
  group_by(dataset) %>% 
  sample_n(10000) %>% 
  arrange(dydx_instrument) %>% 
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = rank, y = dydx_instrument, ymin = dydx_lo, ymax = dydx_hi)) +
  geom_point(aes(colour = dataset)) +
  geom_ribbon(alpha = 0.2) +
  theme_minimal() +
  facet_wrap(~dataset, scales = "free_x") +
  geom_hline(yintercept = 0, linetype = "longdash")


```



```{r plotting_interactions_two}
negative_log_trans <- function(base = exp(1)) {
    trans <- function(x) -log(x, base)
    inv <- function(x) base^(-x)
    trans_new(paste0("negative_log-", format(base)), trans, inv, 
              log_breaks(base = base), 
              domain = c(1e-100, Inf))
}


first_stage_interactions_AE %>% 
  group_by(dataset) %>% 
  sample_n(10000) %>% 
  ggplot(aes(sample = pval_one_neg, colour = dataset)) +
  stat_qq(distribution = qunif) +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  labs(title = "QQ plot of P-values First Stage Interactions",
       subtitle = "Negative Log(10) Scale") +
  facet_wrap(~dataset) +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2)





```

## Maimonides Rule


```{r maim_FS}
first_stage_interactions_maim_5 <- df_final5_cleaned %>% 
  select(classize,
         tipuach,
         c_size,
         c_size_squared,
         func1) %>% 
  run_first_stage_interactions_fast(dataset = .,
                                    dependent_variable = "classize",
                                    instrument = "func1") %>% 
  mutate(dataset = "Fifth Grade") %>% 
  select(dydx_instrument, SE_dydx_instrument, pval_one_neg, pval_holm,  dataset, dydx_lo, dydx_hi)

first_stage_interactions_maim_4 <- df_final4_cleaned %>% 
  select(classize,
         tipuach,
         c_size,
         c_size_squared,
         func1) %>% 
  run_first_stage_interactions_fast(dataset = .,
                                   dependent_variable = "classize",
                                   instrument = "func1") %>% 
  mutate(dataset = "Fourth Grade") %>% 
  select(dydx_instrument, SE_dydx_instrument, pval_one_neg, pval_holm, dataset, dydx_lo, dydx_hi)

first_stage_interactions_maimonides <- bind_rows(first_stage_interactions_maim_5, first_stage_interactions_maim_4) 
```



```{r maim_plot}

first_stage_interactions_maimonides %>% 
  filter(pval_one_neg < 0.1 | pval_holm < 0.1)

first_stage_interactions_maimonides %>% 
  ggplot(aes(x = dydx_instrument, fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")

first_stage_interactions_maimonides %>% 
  ggplot(aes(x = pval_one_neg, fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")

first_stage_interactions_maimonides %>% 
  group_by(dataset) %>% 
  arrange(dydx_instrument) %>% 
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = rank, y = dydx_instrument, ymin = dydx_lo, ymax = dydx_hi)) +
  geom_point(aes(colour = dataset)) +
  geom_ribbon(alpha = 0.2) +
  theme_minimal() +
  facet_wrap(~dataset, scales = "free_x") +
  geom_hline(yintercept = 0, linetype = "longdash")

first_stage_interactions_maimonides %>% 
  group_by(dataset) %>% 
  ggplot(aes(sample = pval_one_neg, colour = dataset)) +
  stat_qq(distribution = qunif) +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  labs(title = "QQ plot of P-values First Stage Interactions",
       subtitle = "Negative Log(10) Scale") +
  facet_wrap(~dataset) +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2)


```

## Autor Dorn Hanson

```{r adh_fs}
df_china <- read_dta("Autor Dorn and Hanson China.dta")
first_stage_interactions_ADH_manu <- df_china %>% 
  select(d_sh_empl_mfg,
         d_tradeusch_pw,
         d_tradeotch_pw_lag,
         l_shind_manuf_cbp,
         starts_with("reg"),
         l_sh_popedu_c,
         l_sh_popfborn,
         l_sh_empl_f,
         l_sh_routine33,
         l_task_outsource,
         t2) %>% 
  run_first_stage_interactions_fast(dataset = .,
                                    dependent_variable = "d_tradeusch_pw",
                                    instrument = "d_tradeotch_pw_lag",
                                    weights = df_china$timepwt48) %>% 
  mutate(dataset = "only")


```



```{r ADH_plot}
first_stage_interactions_ADH_manu %>% nrow()

first_stage_interactions_ADH_manu %>% 
  filter(pval_holm < 0.05)

first_stage_interactions_ADH_manu %>% 
  ggplot(aes(x = dydx_instrument, fill = dataset)) +
  geom_histogram() 

first_stage_interactions_ADH_manu %>% 
  ggplot(aes(x = pval_one_neg, fill = dataset)) +
  geom_histogram() 

first_stage_interactions_ADH_manu %>% 
  arrange(dydx_instrument) %>% 
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = rank, y = dydx_instrument, ymin = dydx_lo, ymax = dydx_hi)) +
  geom_point(aes(colour = dataset)) +
  geom_ribbon(alpha = 0.2) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "longdash")

first_stage_interactions_ADH_manu %>% 
  ggplot(aes(sample = pval_one_neg, colour = dataset)) +
  stat_qq(distribution = qunif) +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  labs(title = "QQ plot of P-values First Stage Interactions",
       subtitle = "Negative Log(10) Scale") +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2)
```


# Random Forest


Write up about methodology and simulation performance

## Angrist and Evans

```{r random_forest_run}
library(grf)
N_obs <- 10000
df_rf_AE_80 <- AE_data_80 %>% 
  sample_n(N_obs)

X_matrix_AE_80 <- df_rf_AE_80 %>% 
  select(-samesex, -morekids) %>% 
  as.matrix()

forest_first_stage_AE_80 <- causal_forest(X = X_matrix_AE_80,
                                    Y = df_rf_AE_80$morekids,
                                    W = df_rf_AE_80$samesex,
                                    num.trees = 4000) 
tau_hat_oob_AE_80 <- predict(forest_first_stage_AE_80, estimate.variance = TRUE) %>%
  as_tibble() %>% 
  mutate(dataset = "1980")

## Now 90s

df_rf_AE_90 <- AE_data_90 %>% 
  sample_n(N_obs)

X_matrix_AE_90 <- df_rf_AE_90 %>% 
  select(-samesex, -morekids) %>% 
  as.matrix()

forest_first_stage_AE_90 <- causal_forest(X = X_matrix_AE_90,
                                          Y = df_rf_AE_90$morekids,
                                          W = df_rf_AE_80$samesex,
                                          num.trees = 4000)
tau_hat_oob_AE_90 <- predict(forest_first_stage_AE_90, estimate.variance = TRUE) %>% 
  as_tibble() %>% 
  mutate(dataset = "1990")

tau_hat_oob_AE <- bind_rows(tau_hat_oob_AE_90,
                            tau_hat_oob_AE_80)


```

```{r random_forest_plots}


ggplot(tau_hat_oob_AE,
       aes(x = predictions,
           fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset)

tau_hat_results <- tau_hat_oob_AE %>%
  group_by(dataset) %>% 
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         critical_value = qt(0.95, N_obs - 13, lower.tail = FALSE),
         pval = pt(t_stat, df = N_obs - 13),
         pval_holm = p.adjust(pval, method = "holm"),
         reject_H0 = ifelse(pval_holm < 0.05, 1, 0),
         rank = row_number())

tau_hat_results %>% 
  ggplot(aes(x = rank, y = predictions, ymin = prediction_lo, ymax = prediction_hi)) + 
  geom_point(aes(colour = dataset)) +
  geom_ribbon(alpha = 0.1) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "longdash") +
  facet_wrap(~dataset) 

tau_hat_results %>% 
  filter(reject_H0 == 1) 


tau_hat_results %>% 
  ggplot(aes(sample = pval, colour = dataset)) +
  stat_qq(distribution = qunif)  +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2) +
  labs(title = "QQ plot of P-values",
       subtitle = "Negative Log(10) Scale") +
  facet_wrap(~dataset)
  
```


## Maimonides Rule



```{r rf_run_maimonides}

X_matrix_maim_5 <- df_final5_cleaned %>% 
  select(
         tipuach,
         c_size,
         c_size_squared) %>% 
  as.matrix()

forest_first_stage_maim_5 <- causal_forest(X = X_matrix_maim_5,
                                    Y = df_final5_cleaned$classize,
                                    W = df_final5_cleaned$func1,
                                    num.trees = 4000) 
tau_hat_oob_maim_5 <- predict(forest_first_stage_maim_5, estimate.variance = TRUE) %>%
  as_tibble() %>% 
  mutate(dataset = "Fifth Grade")

## Now fourth grade


X_matrix_maim_4 <- df_final4_cleaned %>% 
  select(
         tipuach,
         c_size,
         c_size_squared) %>% 
  as.matrix()

forest_first_stage_maim_4 <- causal_forest(X = X_matrix_maim_4,
                                    Y = df_final4_cleaned$classize,
                                    W = df_final4_cleaned$func1,
                                    num.trees = 4000) 
tau_hat_oob_maim_4 <- predict(forest_first_stage_maim_4, estimate.variance = TRUE) %>%
  as_tibble() %>% 
  mutate(dataset = "Fourth Grade")


tau_hat_oob_maimonides <- bind_rows(tau_hat_oob_maim_5,
                                    tau_hat_oob_maim_4)
```



```{r rf_plot_maimonides}
ggplot(tau_hat_oob_maimonides,
       aes(x = predictions,
           fill = dataset)) +
  geom_histogram() +
  facet_wrap(~dataset)

tau_hat_results_maimonides <- tau_hat_oob_maimonides %>%
  group_by(dataset) %>% 
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         critical_value = qt(0.95, N_obs - 13, lower.tail = FALSE),
         pval = pt(t_stat, df = N_obs - 13),
         pval_holm = p.adjust(pval, method = "holm"),
         reject_H0 = ifelse(pval_holm < 0.05, 1, 0),
         rank = row_number())

tau_hat_results_maimonides %>% 
  ggplot(aes(x = rank, y = predictions, ymin = prediction_lo, ymax = prediction_hi)) + 
  geom_point(aes(colour = dataset)) +
  geom_ribbon(alpha = 0.1) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "longdash") +
  facet_wrap(~dataset) 

tau_hat_results_maimonides %>% 
  filter(reject_H0 == 1) 


tau_hat_results_maimonides %>% 
  ggplot(aes(sample = pval, colour = dataset)) +
  stat_qq(distribution = qunif)  +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2) +
  labs(title = "QQ plot of P-values",
       subtitle = "Negative Log(10) Scale") +
  facet_wrap(~dataset)
```

## Autor Dorn Hanson
```{r ADH_rf}

ADH_matrix <- df_china %>% 
  select(d_sh_empl_mfg,
         l_shind_manuf_cbp,
         starts_with("reg"),
         l_sh_popedu_c,
         l_sh_popfborn,
         l_sh_empl_f,
         l_sh_routine33,
         l_task_outsource,
         t2) %>% 
  as.matrix()



forest_first_stage_ADH <- causal_forest(X = ADH_matrix,
                                        Y = df_china$d_tradeusch_pw,
                                        W = df_china$d_tradeotch_pw_lag,
                                        num.trees = 4000)

tau_hat_oob_ADH <- predict(forest_first_stage_maim_5, estimate.variance = TRUE) %>%
  as_tibble() %>% 
  mutate(dataset = "Fifth Grade")

```






```{r ADH_rf_plot}

ggplot(tau_hat_oob_ADH,
       aes(x = predictions)) +
  geom_histogram()

tau_hat_results_ADH <- tau_hat_oob_ADH %>%
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         critical_value = qt(0.95, N_obs - 13, lower.tail = FALSE),
         pval = pt(t_stat, df = N_obs - 13),
         pval_holm = p.adjust(pval, method = "holm"),
         reject_H0 = ifelse(pval_holm < 0.05, 1, 0),
         rank = row_number())

tau_hat_results_ADH %>% 
  ggplot(aes(x = rank, y = predictions, ymin = prediction_lo, ymax = prediction_hi)) + 
  geom_point() +
  geom_ribbon(alpha = 0.1) +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "longdash")

tau_hat_results_ADH %>% 
  filter(reject_H0 == 1) 


tau_hat_results_ADH %>% 
  ggplot(aes(sample = pval)) +
  stat_qq(distribution = qunif)  +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() +
  scale_x_continuous(trans=negative_log_trans(10)) +
  scale_y_continuous(trans=negative_log_trans(10)) +
  geom_hline(yintercept = 0.05, linetype = "longdash", alpha = 0.2) +
  labs(title = "QQ plot of P-values",
       subtitle = "Negative Log(10) Scale")
```


# Bayesian Test
write up on bayesian test

## Simulations
## Angrist and Evans
## Maimonides Rule
## Autor Dorn Hanson

# Conclusion

# References
