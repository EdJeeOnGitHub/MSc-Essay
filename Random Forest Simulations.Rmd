---
title: "Random Forest Simulations"
author: "Ed Jee"
date: "23 January 2019"
output:
    html_document:
      code_folding: show
      highlight: tango
      theme: readable
      toc: yes
      toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

# Setting up DGP


Here we create simulated data - the generic set up is:

$$
y_i = \tau_i(z_i) + 5x_{1i} + 4x_{2i}^2 - 10x_{3i} + u_i; \ u_i \sim N(0, 1)
$$

where $\tau_i$ is some heterogeneous function of $z_i$ our instrument. Monotonicity requires that $\frac{\partial\tau_i(z_i)}{\partial z_i} \ge 0\ \forall i\   \textit{or vice versa}$. In the language of Angrist and Imbens/Mostly Harmless $\pi_i \ge 0 \ \forall i \ \textit{or vice versa}$ - we say that the first stage "goes the same way" for every individual $i$.


We also include several other white noise variables - Athey and Imbens do this to demonstrate random forest's superiority over $k$-nearest neighbours.
```{r}
library(dplyr)
library(ggplot2)
library(tibble)

create_sim_data <- function(n = 1000, n_variables = 10, binary = TRUE, prop_treated = 0.5, error_sd = 1,Z_FUN = runif, z_a = NULL, z_b = NULL, tau_FUN = rnorm, ...){
  X <- matrix(rnorm(n*n_variables), n, n_variables)
  if (binary == TRUE){
    Z <- rbinom(n, 1, prop_treated)
  } else {
    Z <- Z_FUN(n, z_a, z_b)
  }
    
  tau <- tau_FUN(n = n, ...)
  Y <- tau*Z + X[, 1]*5 + (X[, 2]^2)*4 + X[, 3]*(-10) + rnorm(n, sd = error_sd)
  return(list("X" = X, "Z" = Z, "tau" = tau, "Y" = Y))
} 


binary_complier_defier <- function(n, beta_complier, beta_defier, prop_defier){
  defier_status <- rbinom(n = n, size = 1, prob = prop_defier)
  tau <- beta_defier * defier_status + beta_complier * (1 - defier_status)
  return(tau)
}

fake_data <- create_sim_data(n = 10000, tau_FUN = binary_complier_defier, beta_complier = 0.3, beta_defier = -0.5, prop_defier = 0.2)
fake_data_df <- tibble("Y" = fake_data$Y, "Z" = fake_data$Z, "tau" = fake_data$tau) %>% 
  bind_cols(as_tibble(fake_data$X))
```


# Binary Instrument and Binary Heterogeneous Treatment Effect

Here we can see that there are both defiers and compliers present in the data:

```{r}
ggplot(fake_data_df, aes(x = tau)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = "Histogram of Treatment Effects")
```

```{r}
ggplot(fake_data_df %>% 
         sample_frac(0.1), aes(x = Z, y = tau*Z)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Treatment Effect vs Instrument")
```


### Random Forest


We run a causal random forest and plot the histogram of the heterogeneous treatment effects estimated:
```{r}
library(grf)
first_stage_forest = causal_forest(X = fake_data$X, Y = fake_data$Y, W = fake_data$Z, num.trees = 4000)

# Estimate treatment effects for the training data using out-of-bag prediction.
tau_hat_oob = predict(first_stage_forest, estimate.variance = TRUE)
ggplot(tau_hat_oob, aes(x = predictions)) +
  geom_histogram()
```



Now we compare the true $\tau_i$ to estimated $\hat{\tau_i}$ - the 45 degree line indicates equality between predictions and true values:
```{r}
tau_comparison_df <- bind_cols(fake_data_df, tau_hat_oob)
ggplot(tau_comparison_df, aes(x = tau, y = predictions)) +
  geom_point() +
  geom_abline(slope = 1) +
  theme_minimal()
```
Clearly in the case of discretely heterogeneous $\tau$ the random forest isn't too hot. However we can still check if it's useful for our purpose of testing monotonicity.


Here we compare the distribution of true $\tau_i$ and predicted $\hat{\tau_i}$, the joint density/histogram is overlaid in gray:
```{r}
library(tidyr)
tau_comparison_df_plot <- tau_comparison_df %>% 
  select(tau, predictions) %>% 
  gather(variable, value)

tau_comparison_df_subset <- tau_comparison_df_plot %>% 
  select(-variable)

tau_comparison_df_plot %>% 
  ggplot(aes(x = value, fill = variable)) +
  geom_histogram(data = tau_comparison_df_subset,
                 fill = "grey",
                 alpha = 0.8) +
  geom_histogram(colour = "black") +
  facet_wrap(~variable) +
  theme_bw()
```


Now, we order the predicted heterogeneous treatment effects by rank and compute confidence intervals:
```{r}
tau_comparison_df %>% 
  select(tau, predictions, variance.estimates) %>% 
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         rank = row_number()) %>% 
  ggplot(aes(x = rank, y = predictions, ymax = prediction_hi, ymin = prediction_lo)) +
  geom_point() +
  geom_ribbon(alpha = 0.1) +
  theme_minimal() +
  geom_hline(yintercept = 0)
```

An initial glance suggests that there are heterogeneous effects estimated that are significantly below 0 - however, do we need to think about the false discovery rate?



# Binary Instrument and Continuous Heterogeneous Treatment Effect


Creating data, treatment effect of instrument now varies continuously (actually a normal r.v. with mean 0.2 and sd 0.2):
```{r}
rm(list = ls())
create_sim_data <- function(n = 1000, n_variables = 10, binary = TRUE, prop_treated = 0.5, error_sd = 1,Z_FUN = runif, z_a = NULL, z_b = NULL, tau_FUN = rnorm, ...){
  X <- matrix(rnorm(n*n_variables), n, n_variables)
  if (binary == TRUE){
    Z <- rbinom(n, 1, prop_treated)
  } else {
    Z <- Z_FUN(n, z_a, z_b)
  }
    
  tau <- tau_FUN(n = n, ...)
  Y <- tau*Z + X[, 1]*5 + (X[, 2]^2)*4 + X[, 3]*(-10) + rnorm(n, sd = error_sd)
  return(list("X" = X, "Z" = Z, "tau" = tau, "Y" = Y))
} 
continuous_fake_data <- create_sim_data(n = 10000, binary = TRUE, tau_FUN = rnorm, mean = 0.2, sd = .2)
continuous_fake_df <- tibble("Y" = continuous_fake_data$Y, "Z" = continuous_fake_data$Z, "tau" = continuous_fake_data$tau) %>% 
  bind_cols(as_tibble(continuous_fake_data$X))


ggplot(continuous_fake_df, aes(x = tau)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = "Histogram of Treatment Effects")
ggplot(continuous_fake_df, aes(x = Z, y = tau*Z)) +
  geom_point() +
  theme_minimal()
```

### Random Forest

```{r}
first_stage_forest = causal_forest(X = continuous_fake_data$X, Y = continuous_fake_data$Y, W = continuous_fake_data$Z, num.trees = 4000)

# Estimate treatment effects for the training data using out-of-bag prediction.
tau_hat_oob = predict(first_stage_forest, estimate.variance = TRUE)
ggplot(tau_hat_oob, aes(x = predictions)) +
  geom_histogram()
```



```{r}
tau_comparison_df <- bind_cols(continuous_fake_df, tau_hat_oob)
ggplot(tau_comparison_df, aes(x = tau, y = predictions)) +
  geom_point() +
  geom_abline(slope = 1) +
  theme_minimal()
```


```{r}
tau_comparison_df_plot <- tau_comparison_df %>% 
  select(tau, predictions) %>% 
  gather(variable, value)

tau_comparison_df_subset <- tau_comparison_df_plot %>% 
  select(-variable)

tau_comparison_df_plot %>% 
  ggplot(aes(x = value, fill = variable)) +
  geom_histogram(data = tau_comparison_df_subset,
                 fill = "grey",
                 alpha = 0.8) +
  geom_histogram(colour = "black") +
  facet_wrap(~variable) +
  theme_bw()
```



```{r}
tau_comparison_df %>% 
  select(tau, predictions, variance.estimates) %>% 
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         rank = row_number()) %>% 
  ggplot(aes(x = rank, y = predictions, ymax = prediction_hi, ymin = prediction_lo)) +
  geom_point() +
  geom_ribbon(alpha = 0.1) +
  theme_minimal() +
  geom_hline(yintercept = 0)
```



# Comparisons with the Bootstrap 
```{r}
library(broom)
library(modelr)
library(purrr)
boots <- continuous_fake_df %>%
  select(-tau) %>% 
  bootstrap(n = 1000)
ols_on_boot <- function(strap){
  lm(Y ~ . , data = strap)
}
boot_models <- boots %>%
  mutate(model = map(strap, ols_on_boot),
         coef_info = map(model, tidy))

boot_coefs <- boot_models %>%
  unnest(coef_info)

boot_coefs %>%
  filter(term == "Z") %>%
  ggplot(aes(x = estimate)) +
  geom_histogram()

```



combining forest with bootstrap histograms
```{r}
tau_boot_coefs <- boot_coefs %>%
  filter(term == "Z") %>%
  select(value = estimate) %>%
  mutate(variable = "boot")

tau_p <- tau_comparison_df_plot %>%
  bind_rows(tau_boot_coefs)
tau_p %>%
  ggplot(aes(x = value, fill = variable)) +
  geom_density() +
  facet_wrap(~variable)
```

```{r}
tau_p %>% 
  group_by(variable) %>% 
  summarise(tau_mean = mean(value),
            tau_sd = sd(value),
            tau_IQR = IQR(value),
            tau_min = min(value),
            tau_max = max(value),
            tau_range = tau_max - tau_min)
```


# Creating a Test

```{r}
tau_comparison_df %>% 
  select(tau, predictions, variance.estimates) %>% 
  arrange(predictions) %>% 
  mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         critical_value = qt(0.95, 990, lower.tail = FALSE),
         reject_h0 = ifelse(t_stat < critical_value, 1, 0)) %>% 
  filter(critical_value == 1)
```



```{r}
run_one_test <- function(simulated_data){
  simulated_data_df <- tibble("Y" = simulated_data$Y, "Z" = simulated_data$Z, "tau" = simulated_data$tau) %>% 
  bind_cols(as_tibble(simulated_data$X))
  print(mean(simulated_data_df$tau))
  forest_first_stage <- causal_forest(X = simulated_data$X, Y = simulated_data$Y, W = simulated_data$Z)
  tau_hat_oob = predict(forest_first_stage, estimate.variance = TRUE)
  tau_comparison_df <- bind_cols(simulated_data_df, tau_hat_oob)
  tau_comparison_df <- tau_comparison_df %>% 
    select(tau, predictions, variance.estimates) %>% 
    mutate(sigma_hat = sqrt(variance.estimates),
         prediction_lo = predictions - 1.96*sigma_hat,
         prediction_hi = predictions + 1.96*sigma_hat,
         t_stat = predictions / sigma_hat,
         critical_value = qt(0.95, 990, lower.tail = FALSE),
         reject_h0 = ifelse(t_stat < critical_value, 1, 0))
  return(tau_comparison_df)
}

ed <- run_one_test(simulated_data = create_sim_data(n = 100000, tau_FUN = binary_complier_defier, beta_complier = 0.2, beta_defier = 0.05, prop_defier = 0.1))
 
```


```{r}
ed %>% 
  filter(reject_h0 == 1)
```

```{r}
ed %>% ggplot(aes(x = tau)) +
  geom_histogram()
```


```{r}
ed %>% 
  arrange(predictions) %>% 
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = rank, y = predictions, ymin = prediction_lo, ymax = prediction_hi)) +
  geom_point(aes(colour = factor(reject_h0))) +
  geom_ribbon(alpha = 0.1)
```

